{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52eb587b",
   "metadata": {},
   "source": [
    "### Bayesian optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn\n",
    "import GPy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1248d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Toy_function:\n",
    "    '''\n",
    "    This function is called Ackley, it's a standard benchmark function for optimizers.\n",
    "    '''\n",
    "    def __init__(self, ndim):\n",
    "        self.ndim = ndim\n",
    "        self.a = 20\n",
    "        self.b = 0.2\n",
    "        self.c = 2 * np.pi\n",
    "        self.domain = [[-32.768, 32.768] for ii in range(self.ndim)]\n",
    "        \n",
    "    def f(self, X):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise TypeError(\"X have to be a 1D numpy array !\")\n",
    "        if X.ndim == 2 and X.shape[0] != 1:\n",
    "            raise ValueError(\"Pass input values one by one\")\n",
    "        if X.ndim == 2:\n",
    "            X = X.reshape(-1)\n",
    "            \n",
    "        if X.shape[0] != self.ndim:\n",
    "            raise ValueError\n",
    "        res = - self.a * np.exp(-self.b * np.sqrt((1/self.ndim) * np.sum(X**2))) - np.exp((1/self.ndim) * np.sum(np.cos(self.c * X))) + self.a + np.exp(1)\n",
    "        return np.array(res).reshape(-1, 1)\n",
    "\n",
    "# how to use it ? \n",
    "function_to_optimize = Toy_function(ndim=2)\n",
    "function_to_optimize.f(np.array([[0, 0]]).reshape(-1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f6f8ce",
   "metadata": {},
   "source": [
    "### Random optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38710c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Here\n",
    "## Input : \n",
    "#####\n",
    "## f: callable \n",
    "## domain: input space (bounds)\n",
    "## n: number of drawn element\n",
    "#####\n",
    "## Output : \n",
    "## X : np.ndarray (n, dim_domain)\n",
    "## Y : np.ndarray (n,)\n",
    "#####\n",
    "def draw_random(f, domain, n):\n",
    "    # TODO \n",
    "    return X, Y\n",
    "\n",
    "# How to use it\n",
    "nb_init = 5\n",
    "X_init, Y_init = draw_random(f, domain, nb_init)\n",
    "print(X_init)\n",
    "print(Y_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eaf784",
   "metadata": {},
   "source": [
    "### Create your surrogate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "## See GPy GPRegression documentation\n",
    "## Hint :\n",
    "## - You need X, Y, kernel \n",
    "## - GP supports unnormalized data but it can mislead the fitting process (StandardScaler is a good option)\n",
    "## - A standard kernel for a smooth regression is Matern52, see GPy.kern.Matern52\n",
    "## - You need to optimize hyper parameters and following the log marginal likelihood, see model.optimize_restarts\n",
    "class My_model:\n",
    "    def __init__(self, input_dim, num_restarts_optim_model=10):\n",
    "        # TODO\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        # TODO\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "        return mu, sig\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def get_kernel(self):\n",
    "        return self.kernel # or self.model.kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126e5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mymodel = My_model(input_dim=len(domain))\n",
    "#mymodel.fit(X_init, Y_init)\n",
    "#mymodel.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f29fa4",
   "metadata": {},
   "source": [
    "### Create your acqusition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33829f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "## Minimization\n",
    "class LCB_acquisition:\n",
    "    def __init__(self, model, exploration_param=0.05):\n",
    "        # TODO\n",
    "        \n",
    "        \n",
    "    def acq(self, X):\n",
    "        # TODO\n",
    "        return acq_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b52ba",
   "metadata": {},
   "source": [
    "### Optimize your acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "def optimize_acquisition(acq, domain):\n",
    "    \n",
    "    # Setup f_to_optimize with a wrapper for convenience\n",
    "    def f_wrapper(acq_obj):\n",
    "        def f(X):\n",
    "            return - acq_obj.acq(X).reshape(-1) # maximize <==> minimize the negative\n",
    "        return f\n",
    "    \n",
    "    f_to_optim = f_wrapper(acq)\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return x_opt, f_opt\n",
    "        \n",
    "#acquisition = LCB_acquisition(mymodel)\n",
    "#res = optimize_acquisition(acquisition, domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851486f",
   "metadata": {},
   "source": [
    "### All together : Bayesian optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67317229",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "def bayesian_optimization(func, domain, budget=50):\n",
    "    # TODO\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "function = Toy_function(ndim=2)\n",
    "f = function.f\n",
    "domain = function.domain\n",
    "\n",
    "X, Y = bayesian_optimization(f, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1a32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd57ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "164c9a15",
   "metadata": {},
   "source": [
    "#### Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1afe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_score_evolution(Y):\n",
    "    best_y = []\n",
    "    for yy in Y:\n",
    "        if len(best_y) == 0:\n",
    "            best_y.append(yy)\n",
    "            continue\n",
    "        if yy < best_y[len(best_y) - 1]:\n",
    "            best_y.append(yy)\n",
    "        else:\n",
    "            best_y.append(best_y[len(best_y) - 1])\n",
    "    \n",
    "    plt.plot(best_y)\n",
    "    plt.show()\n",
    "plot_best_score_evolution(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e20900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_to_optimize():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.list_names_param = [\n",
    "            \"min_samples_split\",\n",
    "            \"min_samples_leaf\",\n",
    "            \"min_weight_fraction_leaf\",\n",
    "            \"max_features\",\n",
    "            \"max_samples\"]\n",
    "        self.domain = [\n",
    "            (10e-5, 1 - 10e-5), \n",
    "            (10e-5, 0.5),\n",
    "            (10e-5, 0.5),\n",
    "            (10e-5, 1 - 10e-5), \n",
    "            (10e-3, 1 - 10e-5) #--\n",
    "        ]\n",
    "        df = pd.read_csv(\"./laptop_price_clean.csv\")\n",
    "        X = df.drop(columns=['Price_euros'])\n",
    "        self.X_data = pd.get_dummies(X)\n",
    "        self.Y_data = np.log(df['Price_euros'])\n",
    "\n",
    "    '''\n",
    "        Parameters_array : numpy array containing 0+ values correspind to the parameters of the RF (if Ã˜ or None, default values are used)\n",
    "        Return: sklearn readable parameters\n",
    "    '''\n",
    "    def preprocess_input_parameters(self, parameters_array):\n",
    "        self.params = {}\n",
    "        for ii in range(parameters_array.shape[0]):\n",
    "            if parameters_array[ii] is not None and (parameters_array[ii] < self.domain[ii][0] or parameters_array[ii] > self.domain[ii][1]):\n",
    "                raise ValueError(\"parameter not in bound\")\n",
    "            if parameters_array[ii] is not None:\n",
    "                self.params[self.list_names_param[ii]] = parameters_array[ii]\n",
    "\n",
    "    def pipeline(self, X_data):\n",
    "        pass\n",
    "    \n",
    "    def f(self, X):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise TypeError(\"X have to be a 1D numpy array !\")\n",
    "        if X.ndim == 2 and X.shape[0] != 1:\n",
    "            raise ValueError(\"Pass input values one by one\")\n",
    "        if X.ndim == 2:\n",
    "            X = X.reshape(-1)\n",
    "            \n",
    "        self.preprocess_input_parameters(X)\n",
    "        kf = sklearn.model_selection.KFold(n_splits=5)\n",
    "        res = []\n",
    "        for train_index, test_index in kf.split(self.X_data):\n",
    "            \n",
    "            X_train = self.X_data.iloc[train_index]\n",
    "            X_test = self.X_data.iloc[test_index]\n",
    "            \n",
    "            Y_train = self.Y_data.iloc[train_index]\n",
    "            Y_test = self.Y_data.iloc[test_index]\n",
    "            \n",
    "            model = ensemble.RandomForestRegressor(**self.params)\n",
    "            model.fit(X_train, Y_train)\n",
    "            Y_pred = model.predict(X_test)\n",
    "            try:\n",
    "                metric = sklearn.metrics.r2_score(Y_test, Y_pred)\n",
    "                res.append(metric)\n",
    "            except ValueError:\n",
    "                print(self.params)\n",
    "\n",
    "        score = np.mean(res) #- np.std(res) \n",
    "        # Maximize (mean(R2 score) - std(R2 score)) <==> Minimize it's negative\n",
    "        return np.array( - score).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68955315",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_optimize = Model_to_optimize()\n",
    "domain = model_to_optimize.domain\n",
    "\n",
    "X, Y = bayesian_optimization(model_to_optimize.f, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb41d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_best_score_evolution(Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
